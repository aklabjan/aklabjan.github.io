<!DOCTYPE html>
<html lang="en">
<script type="text/javascript" async=""
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="Herb Garden Classification System" content="CS766 Project" />
    <meta name="Ana Klabjan, Poulami Paul and Shubhankit Rathore" content="CS766 Project" />
    <title>Herb Garden Classification</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
        type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-block d-lg-none">Herb Garden Classification System</span>
            <!-- <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                    src="assets/img/tomatoes.jpeg" alt="..." /></span> -->
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
                class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Title">Project Details</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#TheProblem">The Problem</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#StateOfTheArt">Current State Of The
                        Art</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Approach/Novelty">Our Approach</a>
                </li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Dataset">Dataset</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage1">Stage 1</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage2">Stage 2</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage3">Stage 3</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage4">Stage 4</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#findings">Findings/Attempts</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#results">Results</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Improvements">Improvements</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Resources">Resources</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#References">References</a></li>
            </ul>
        </div>
    </nav>
    <!-- Page Content-->
    <div class="container-fluid p-0">
        <!-- Title and Names-->
        <section class="resume-section" id="Title">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Herb Garden </span>Classification System</h2>
                <p class="lead mb-5">
                    By Ana Klabjan, Poulami Paul and Shubhankit Rathore
                </p>
                <p class="lead mb-5">CS 766-Spring 2023</p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- TheProblem-->
        <section class="resume-section" id="TheProblem">
            <div class="resume-section-content">
                <h2 class="mb-0">
                    The
                    <span class="text-primary">Problem</span>
                </h2>
                <p class="lead mb-5"> The problem we are trying to solve is to accurately label different herbs in a
                    herb garden when a photo is taken. Specifically, we aim to identify the herbs parsley, thyme,
                    chives, and oregano, while labeling all other herbs as unclassified herb. The goal of this project
                    is to develop a computer vision model that can recognize these specific herbs in an image with high
                    accuracy and efficiency.
                    <br><br>
                    It might be out of curiosity for someone to figure out what kind of herbs they have in their garden
                    or forest. Accurate classification of herb leaves is critical for quality control, authentication,
                    and conservation of medicinal plants. Therefore, the proposed project aims to develop a herb leaf
                    classification system that can identify individual plants within a garden and differentiate between
                    different herb species based on their leaf images.
                </p>
                <h3 class="mb-0">
                    Importance and Motivation
                </h3>
                <p class="lead mb-5">The problem we are trying to solve is important for several reasons. Firstly, it
                    can satisfy the curiosity of people who want to know what kind of herbs they have in their garden or
                    environment. This can be time-consuming and challenging for those without extensive herb knowledge.
                    Secondly, this tool can also be helpful for the food industry, as it can assist with quality control
                    in the production of herb-infused products and reduce the risk of misidentification, which can be
                    dangerous if a misidentified herb is consumed.
                    <br><br>
                    Lastly, accurate identification of herb leaves is crucial for quality control, authentication, and
                    conservation of medicinal plants. The development of a herb leaf classification system that can
                    identify different herb species based on their leaf images can aid in the production of medicines,
                    as well as the identification and preservation of endangered plant species. Additionally, the
                    accurate labeling of herbs in natural environments can provide valuable data for ecological studies,
                    aiding in the conservation and management of natural habitats. Therefore, it is crucial to develop a
                    computer vision model that can accurately and efficiently identify different herb species in images,
                    as it has practical applications for the food industry, herb enthusiasts, medicinal purposes, and
                    environmental research.
                    <br><br>
            </div>
        </section>
        <hr class="m-0" />
        <!-- StateOfTheArt-->
        <section class="resume-section" id="StateOfTheArt">
            <div class="resume-section-content">
                <h2 class="mb-0">Current State Of The Art</h2>
                <p class="lead mb-5">In recent years, herb classification using leaf images has gained significant
                    attention due to its potential applications in the fields of medicine, agriculture, and food
                    industry. The ability to accurately identify different herb species can help in quality control of
                    herbal products, detection of adulteration, and conservation of rare and endangered plant species.
                    <br><br>
                    Most of the existing studies in this area have focused on the extraction of features from images
                    that represent a singular leaf from an herb and their classification using various machine learning
                    algorithms. Texture features, color histograms, and shape descriptors are some of the commonly used
                    features for herb classification[3]. However, with the recent advancements in deep
                    learning, several studies have started exploring the use of convolution neural networks (CNNs) for
                    herb recognition based on leaf images[1].
                    Leaf identification normally use images that taken in the laboratory with sophisticated equipment
                    and a white background. The accuracy of these models have been high.
                    <br><br>
                    Current research has shifted the focus from high-quality plant images to identifying leaves in
                    natural environments. Leaf segmentation in natural environments has different approaches and
                    accuracy levels. There have been many studies that aim to identify leaves based on images taken in
                    their natural environment[2]. These studies all focused on the identification of a
                    singular leaf taken in nature. The accuracy of the models was lower than the models discussed in the
                    prior paragraph that used images taken with sophisticated equipment and a white background, but the
                    models were still considered successful, with high accuracy.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Method-->
        <section class="resume-section" id="Approach">
            <div class="resume-section-content">
                <h2 class="mb-0">Our Approach/Novelty</h2>
                <p class="lead mb-5">Our project intends to expand on the current state-of-the-art. The research for
                    identifying an individual leaf from nature has already been conducted, and our project expands on
                    it. However, we have not found any studies that take an image of numerous plants and segment the
                    image to locate each individual plant. Each segmented plant would then be fed into an identification
                    model, similar to the pre-existing ones. The original image would then be annotated using the model
                    output.
                    <br><br>
                    For plant segmentation, we have developed our own approach. Firstly, we will identify the center of
                    each individual plant and annotate it on the original image. To do this, we will convert the herb
                    garden image to grayscale, perform some noise removal, compute the edges/boundaries, and then
                    compute the "centers" of each object detected by the edges. Through implementation we found this
                    detected individual leaves more than plants as a whole(reference stage 2). Thus leading us to the
                    conclusion we will
                    need to perform some type of clustering.
                    <br><br>
                    For the clusters, we choose to use K-Means to identify the true plant centers from the noisy output
                    provided by Stage 2. In an ideal world we would like to avoid the need for human input of the value
                    “K” but for the time permitted using k-means provided the best possible results. We considered
                    several clustering algorithms including hierarchical clustering. The caveat was even though there is
                    no original specified number of clusters, you would have to identify a criteria on how to decide
                    where to split the clusters in the hierarchy. For our algorithm we predefined K based on manual
                    inspection of each test image declaring K to be the number of distinct plants visible. Using the
                    output from k-means we have the coordinates of what our algorithm believes the true plant centers
                    are. When plotted on the original image we find that the true plants are mostly identified although
                    there are some plant pots and container edges detected that create noise, a falsely identified
                    plant.
                    <br><br>
                    Using the true centers found by the clustering, subimages are extracted to feed into the model. A
                    formula was created to find the size of the subimage output based on the input image size and the
                    number of objects detected in the image. A rectangle of the specified size is placed around each
                    center thus creating the boundaries of the subimage.
                    <br><br>
                    Finally, these segmented image for identified plants are sent to the classification model to predict
                    the type of plant. Each image is then labelled as one of the possible plants as discussed
                    previously, or unidentifiable herb if the confidence score for the prediction is lower than a
                    pre-determined threshold.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Dataset-->
        <section class="resume-section" id="Dataset">
            <div class="resume-section-content">
                <h2 class="mb-0">Dataset</h2>
                <p class="lead mb-5">
                    TO DO
                    <!-- For this project, we were unable to find an already existing dataset. Therefore, we
                    will be creating our own dataset for the training & testing of model from Google Images. To create
                    the training & testing set we will pull 500-1000 images from Google Images for each of our four
                    herb types: parsley, thyme, chives, and oregano, and split the set into 90% and 10% to train &
                    test.
                    <br><br>
                    For the final test set we extracted 20 images of herb gardens. These images were pulled from
                    Instagram as it was a good source of images that resembled the real world input our project is
                    intended for. Creating our own dataset was a big hurdle. In order to eliminate the possibility of
                    incorrectly labeling a plant in the test set when compared to the output from our program we
                    selected images that included labels by the garden. Although this is flawed and has the potential to
                    be incorrectly labeled, we made the assumption the gardener labeled it based on the label of the
                    seed packaging or store label. -->
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 1-->
        <section class="resume-section" id="Stage1">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 1: Blurring Text</h2>
                <p class="lead mb-5">
                    Each image will go through three steps to blurr any text:[4]
                <ol>
                    <li>Identify text in the image and obtain the bounding box coordinates of each text, using
                        Keras-ocr.</li>
                    <li>For each bounding box, apply a mask to tell the algorithm which part of the image we should
                        inpaint.</li>
                    <li>Apply an inpainting algorithm to inpaint the masked areas of the image, resulting in a text-free
                        image, using cv2.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/input.png" alt="Original Image" height="400">
                        <div>Stage 1 Input</div>
                    </div>
                    <div>
                        <img src="./assets/img/noWordsOutput.png" alt="Image with all text Blurried" height="400">
                        <div>Stage 1 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 2-->
        <section class="resume-section" id="Stage2">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 2: Annotate "Centers"</h2>
                <p class="lead mb-5">
                    The image will be processed to create a dataset of object centers to be processed in Stage 3.The is
                    done in four steps:
                <ol>
                    <li>Apply a specified blur filter to the image using skimage.filters. Possible
                        options are Gaussian, median, bilateral, and total variation. We choosing Gaussian based off of
                        trying them and comparing the annotated images.</li>
                    <li>Convert the blurred image to a binary image using skimage.filters.threshold_otsu. Threshold was
                        also based off of trying values and comparing the annotated images.</li>
                    <li>Label connected components in the binary image and compute region properties for each object
                        using measure.label and measure.regionprops respectively. </li>
                    <li>For each plant, plot a red circle on the centroid coordinates using matplotlib.pyplot.plot.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/annotateCenterOutput.png" alt="Image with red dots." height=400>
                        <div>Stage 2 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 3-->
        <section class="resume-section" id="Stage3">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 3: Annotate True Centers</h2>
                <p class="lead mb-5">
                    In this stage, the "centers" found in stage 2 are processed using K-means to find the true center of
                    each plant. Unfortunately, this stage currently requires manual input for the value of K. In the
                    future, we would like to use a clustering method that does not require human input to determine the
                    optimal number of clusters. X-Means is one such algorithm[6]. Due to the current noise after stage 2
                    X-Means does not perform well so we are using K-Means for better results from the segmentation. If
                    we can filter out the noise I believe replacing K-means with X-means would lead to good results and
                    no human input.
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/centerOutput.png" alt="Example image of stage three output." height=400>
                        <div>Stage 3 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 4-->
        <section class="resume-section" id="Stage4">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 4: Extract Subimages Image</h2>
                <p class="lead mb-5">
                    In Stage 4 using the true centers found in stage 3, we extract surrounding pixels to create
                    subimages of each plant.

                    The formula used to calculate the size of the subimages is based off of the original image size and
                    the number of plants in the image.

                </p>
                <p><span class="math display">\[subimage_{size} = \sqrt{\frac{image_{height} *
                        image_{width}}{number\_of\_plants * 1.5}}\]</span></p>
                <p class="lead mb-5">
                    For each x,y cordinate that indicates a plant center a box boundry is created using the following
                    formula:
                </p>
                <p><span class="math display">\[x_{min} = \max(0,x - \lfloor \frac{subimage_{size}}{2} \rfloor)
                        \]</span>
                </p>
                <p><span class="math display">\[x_{max} = \min(image_{width},x + \lfloor \frac{subimage_{size}}{2}
                        \rfloor) \]</span></p>
                <p><span class="math display">\[y_{min} = \max(0,y - \lfloor \frac{subimagegit _{size}}{2} \rfloor)
                        \]</span>
                </p>
                <p><span class="math display">\[y_{max} = \min(image_{height},y + \lfloor \frac{subimage_{size}}{2}
                        \rfloor) \]</span></p>
                <p class="lead mb-5">
                    The box boundries are used to extract the subimages.
                </p>
                <div class="image-container">
                    <div>
                        <div>Stage 4 Output</div>
                        <table>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_0.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_1.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_2.png" width=200 height=200
                                        object-fit=contain></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_3.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_4.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_5.png" width=200 height=200></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_6.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_7.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_8.png" width=200 height=200></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_9.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_10.png" width=200 height=200></td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Findings-->
        <section class="resume-section" id="Findings">
            <div class="resume-section-content">
                <h2 class="mb-0">Findings/Attempts</h2>
                <p class="lead mb-5">
                    Throughout the project, a need to improve noise removal in the process of extracting plant
                    sub-images was identified. In stage 2, basic noise removal was performed using Gaussian blur and
                    thresholding to convert the image to a binary format. However, this approach produced a significant
                    amount of noise in the output, including dots on non-leaf areas.
                    <br></br>
                    Two attempts were made to address this issue, but both failed and require further investigation. The
                    first approach involved filtering the output produced by Stage 2 before using it as input for
                    K-Means in Stage 3. The idea behind this approach was to identify dots associated with actual plants
                    by looking for some variety of green color, while ignoring dots that were unlikely to be part of a
                    plant (such as a pot or a pole).
                    <br></br>
                    The filtering process involved looking up the pixel value of a (X, Y) coordinate in the image,
                    computing the distance to a predetermined value (green_color), and checking if the distance was
                    within a threshold. If the pixel value was close enough to the color of a leaf, it was included in
                    the Stage 3 clustering computation. Otherwise, it was filtered out. An outline can be found below:
                </p>
                <img src="./assets/img/filter.png" width=700>
                <p class="lead mb-5">
                    However, this approach had a major limitation in that it required a better way to determine the
                    value of green_color. Ideally, green_color would be the average value of all pixels in the image
                    that correspond to being part of a plant. However, this value may not be uniform across multiple
                    images due to various factors such as lighting. Further research is needed to determine the correct
                    value for green_color in order to make this filter effective.
                    <br></br>
                    Another attempt to address the issue of noise was made by exploring the possibility of filtering the
                    sub-images produced by Stage 4. The idea behind this attempt was similar to the previous approach,
                    based on the hypothesis that sub-images of plants and sub-images of noise would contain
                    distinguishable pixel values. A sub-image of a plant would typically contain a large number of green
                    pixels, while a sub-image of noise would have few or no green pixels.
                    <br></br>
                    This approach was still in the exploration phase and had not been implemented. The initial
                    exploration involved analyzing the green channel of each sub-image. For each image, the average
                    value of the green channel was calculated for each sub-image. It was found that sub-images
                    containing plants tended to have a similar average value within a range, while the average value of
                    sub-images containing noise fell outside that range. For example, when the average pixel value of
                    the green channel was calculated for each sub-image under Stage 4, the output was as follows:
                    <br></br>
                    Plant Sub-Images = [158.21, 162.95, 150.95, 155.71, 156.73, 163.50, 150.59, 163.27, 148.78]
                    <br></br>
                    Noisy Sub-Images = [184.56, 142.91]
                    <br></br>
                    As shown in this output, sub-images with an average green channel pixel value between 148-163 were
                    identified as containing plants, while sub-images with an average green channel pixel value outside
                    this range matched up with the two images of the white posts.
                    <br></br>
                    However, the average value range of the green channel varied from image to image due to factors such
                    as lighting, which meant that each image would need to have its own range. This led to further
                    exploration, including plotting histograms of pixel values for each color channel on every
                    sub-image. Although a lot of information was present in these histograms, there were no distinct
                    features that stood out.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Results-->
        <section class="resume-section" id="results">
            <div class="resume-section-content">
                <h2 class="mb-0">Results</h2>
                <p class="lead mb-5">
                    TO DO
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Discussion-->
        <section class="resume-section" id="Improvements">
            <div class="resume-section-content">
                <h2 class="mb-0">Improvements</h2>
                <p class="lead mb-5">
                    TO DO
                    <!-- One of the main limitation of our current set up is that we only train our model on four herbs, when
                    in reality there are thousands of different herbs. At minimum it would be great to expand the model
                    to at least cover all of the commonly used spices and herbs used world wide in cooking. This would
                    make the application in the food industry much more useful.
                    <br></br>
                    In an ideal world, we would also use completely different images in training our model. Ideally we
                    would have enough pre-labeled images of herb gardens that would allow us to segment the image and
                    use the sub-images as the train and test set in our model as the model currently uses high quality
                    images of the herbs usually against a white background, this doesn't match the segmented images we
                    are feeding into the model for labeling.
                    <br></br>
                    Another challenge that we aim to tackle if time permits is to successfully segment the mixed leaf
                    scenario and identify the plants. In reality, the image of a garden could contain the interleaved
                    leaves into each other if plants are nearby. Current approach might find it difficult to segment
                    this scenario into two or multiple separate images. Hence, it would be a challenging task to develop
                    a model for identification of leaves in such a case. -->
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Resources-->
        <section class="resume-section" id="Resources">
            <div class="resume-section-content">
                <h2 class="mb-0">Resources</h2>
                <ul class="lead mb-5">
                    <li>
                        <a href="https://github.com/aklabjan/herbDetectionProject">Github Repository</a>
                    </li>
                    <li>
                        <a
                            href="https://docs.google.com/presentation/d/1HoOTOZAkJhqP111lgKF9xl0XaGWU28aQP1xfZ8hRzCA/edit?usp=sharing">Presentation</a>
                    </li>
                </ul>
            </div>
        </section>
        <!-- References-->
        <section class="resume-section" id="References">
            <div class="resume-section-content">
                <h2 class="mb-0">References</h2>
                <ol class="lead mb-5">
                    <li>
                        <div id="ref-8687165" class="csl-entry" role="doc-biblioentry">
                            Liu, Shupeng, Weiyang Chen, and Xiangjun Dong. 2018. <span>“Automatic
                                Classification of Chinese Herbal Based on Deep Learning Method.”</span>
                            In <em>2018 14th International Conference on Natural Computation, Fuzzy
                                Systems and Knowledge Discovery (ICNC-FSKD)</em>, 235–38. <a
                                href="https://doi.org/10.1109/FSKD.2018.8687165">https://doi.org/10.1109/FSKD.2018.8687165</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-8350775" class="csl-entry" role="doc-biblioentry">
                            Mareta, Affix, Indah Soesanti, and Oyas Wahyunggoro. 2018. <span>“Herbal
                                Leaf Classification Using Images in Natural Background.”</span> In
                            <em>2018 International Conference on Information and Communications
                                Technology (ICOIACT)</em>, 612–16. <a
                                href="https://doi.org/10.1109/ICOIACT.2018.8350775">https://doi.org/10.1109/ICOIACT.2018.8350775</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-9239938" class="csl-entry" role="doc-biblioentry">
                            Muneer, Amgad, and Suliman Mohamed Fati. 2020. <span>“Efficient and
                                Automated Herbs Classification Approach Based on Shape and Texture
                                Features Using Deep Learning.”</span> <em>IEEE Access</em> 8: 196747–64.
                            <a
                                href="https://doi.org/10.1109/ACCESS.2020.3034033">https://doi.org/10.1109/ACCESS.2020.3034033</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-removeText" class="csl-entry" role="doc-biblioentry">
                            "Remove Text from Images Using cv2 and Keras-ocr." <em>Towards Data Science</em>. Accessed
                            April 19, 2023. <a
                                href="https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4">https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-1" class="csl-entry" role="doc-biblioentry">
                            "Deep leaf: Mask R-CNN based leaf detection and segmentation from digitized herbarium
                            specimen images." <em>Pattern Recognition Letters</em>, vol. 150, no. 1, 2021, pp. 88-95. <a
                                href="https://doi.org/10.1016/j.patrec.2021.07.003">https://doi.org/10.1016/j.patrec.2021.07.003</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-2" class="csl-entry" role="doc-biblioentry">
                            Novikov, A., 2019. PyClustering: Data Mining Library. <em>Journal of Open Source
                                Software</em>, 4(36), p.1230. Available at: <a
                                href="http://dx.doi.org/10.21105/joss.01230">http://dx.doi.org/10.21105/joss.01230</a>.
                        </div>
                    </li>
                </ol>
            </div>
        </section>
        <hr class="m-0" />
    </div>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>