<!DOCTYPE html>
<html lang="en">
<script type="text/javascript" async=""
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="Herb Leaf Classification System" content="CS766 Project" />
    <meta name="Ana Klabjan, Poulami Paul and Shubhankit Rathore" content="CS766 Project" />
    <title>CS766 Project</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
        type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-block d-lg-none">Herb Leaf Classification System</span>
            <!-- <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                    src="assets/img/tomatoes.jpeg" alt="..." /></span> -->
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
                class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Title">Project Details</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#TheProblem">The Problem</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#StateOfTheArt">Current State Of The
                        Art</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Approach/Novelty">Our Approach</a>
                </li>
                <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Implementation">Implementation
                        Overview</a></li> -->
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage1">Stage 1</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage2">Stage 2</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage3">Stage 3</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage4">Stage 4</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Improvements">Improvements</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Resources">Resources</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#References">References</a></li>
            </ul>
        </div>
    </nav>
    <!-- Page Content-->
    <div class="container-fluid p-0">
        <!-- Title and Names-->
        <section class="resume-section" id="Title">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Herb Leaf </span>Classification System</h2>
                <p class="lead mb-5">
                    By Ana Klabjan, Poulami Paul and Shubhankit Rathore
                </p>
                <p class="lead mb-5">CS 766-Spring 2023</p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- TheProblem-->
        <section class="resume-section" id="TheProblem">
            <div class="resume-section-content">
                <h2 class="mb-0">
                    The
                    <span class="text-primary">Problem</span>
                </h2>
                <p class="lead mb-5"> The problem we are trying to solve is to accurately label different herbs in a
                    herb garden when a photo is taken. Specifically, we aim to identify the herbs parsley, thyme,
                    chives, and oregano, while labeling all other herbs as unclassified herb. The goal of this project
                    is to develop a computer vision model that can recognize these specific herbs in an image with high
                    accuracy and efficiency.
                    <br><br>
                    It might be out of curiosity for someone to figure out what kind of herbs they have in their garden
                    or forest. Accurate classification of herb leaves is critical for quality control, authentication,
                    and conservation of medicinal plants. Therefore, the proposed project aims to develop a herb leaf
                    classification system that can identify individual plants within a garden and differentiate between
                    different herb species based on their leaf images.
                </p>
                <h3 class="mb-0">
                    Importance and Motivation
                </h3>
                <p class="lead mb-5">The problem we are trying to solve is important for several reasons. Firstly, it
                    can satisfy the curiosity of people who want to know what kind of herbs they have in their garden or
                    environment. This can be time-consuming and challenging for those without extensive herb knowledge.
                    Secondly, this tool can also be helpful for the food industry, as it can assist with quality control
                    in the production of herb-infused products and reduce the risk of misidentification, which can be
                    dangerous if a misidentified herb is consumed.
                    <br><br>
                    Lastly, accurate identification of herb leaves is crucial for quality control, authentication, and
                    conservation of medicinal plants. The development of a herb leaf classification system that can
                    identify different herb species based on their leaf images can aid in the production of medicines,
                    as well as the identification and preservation of endangered plant species. Additionally, the
                    accurate labeling of herbs in natural environments can provide valuable data for ecological studies,
                    aiding in the conservation and management of natural habitats. Therefore, it is crucial to develop a
                    computer vision model that can accurately and efficiently identify different herb species in images,
                    as it has practical applications for the food industry, herb enthusiasts, medicinal purposes, and
                    environmental research.
                    <br><br>
            </div>
        </section>
        <hr class="m-0" />
        <!-- StateOfTheArt-->
        <section class="resume-section" id="StateOfTheArt">
            <div class="resume-section-content">
                <h2 class="mb-0">Current State Of The Art</h2>
                <p class="lead mb-5">In recent years, herb classification using leaf images has gained significant
                    attention due to its potential applications in the fields of medicine, agriculture, and food
                    industry. The ability to accurately identify different herb species can help in quality control of
                    herbal products, detection of adulteration, and conservation of rare and endangered plant species.
                    <br><br>
                    Most of the existing studies in this area have focused on the extraction of features from images
                    that represent a singular leaf from an herb and their classification using various machine learning
                    algorithms. Texture features, color histograms, and shape descriptors are some of the commonly used
                    features for herb classification[3]. However, with the recent advancements in deep
                    learning, several studies have started exploring the use of convolution neural networks (CNNs) for
                    herb recognition based on leaf images[1].
                    Leaf identification normally use images that taken in the laboratory with sophisticated equipment
                    and a white background. The accuracy of these models have been high.
                    <br><br>
                    Current research has shifted the focus from high-quality plant images to identifying leaves in
                    natural environments. Leaf segmentation in natural environments has different approaches and
                    accuracy levels. There have been many studies that aim to identify leaves based on images taken in
                    their natural environment[2]. These studies all focused on the identification of a
                    singular leaf taken in nature. The accuracy of the models was lower than the models discussed in the
                    prior paragraph that used images taken with sophisticated equipment and a white background, but the
                    models were still considered successful, with high accuracy.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Method-->
        <section class="resume-section" id="Approach">
            <div class="resume-section-content">
                <h2 class="mb-0">Our Approach/Novelty</h2>
                <p class="lead mb-5">Our project intends to expand on the current state-of-the-art. The research for
                    identifying an individual leaf from nature has already been conducted, and our project expands on
                    it. However, we have not found any studies that take an image of numerous plants and segment the
                    image to locate each individual plant. Each segmented plant would then be fed into an identification
                    model, similar to the pre-existing ones. The original image would then be annotated using the model
                    output.
                    <br><br>
                    For plant segmentation, we have developed our own approach. Firstly, we will identify the center of
                    each individual plant and annotate it on the original image. To do this, we will convert the herb
                    garden image to grayscale, perform some noise removal, compute the edges/boundaries, and then
                    compute the "centers" of each object detected by the edges. Through implementation we found this
                    detected individual leaves more than plants as a whole. Thus leading us to the conclusion we will
                    need to perform some type of clustering.
                    <br><br>
                    Clustering will be used to find the true center of each plant instead of the noise and leaves. For
                    clustering, the goal was to find an algorithm that did not involve human input like k-means which
                    requires K to be specified. We considered several clustering algorithms including Hierarchical
                    clustering. The caviet was even though there is no original specified number of clusters, you would
                    have to identify a criteria on how to decide where to split the clusters in the hierarchy. For the
                    purpose of time we used K-means and predefined K based on manual inspection of each test image. If
                    there was additonal time we would create our own clustering algorith, one that would use the number
                    of points within an area (dense areas are likely plants due to many leaves) as well as distance
                    between these dense areas to identity one plant from another. Creating our own clustering algorithm
                    would hopefully elimate the false centers that aren't plants but noise. Another method to elimate
                    the false centers, would be preprocesses the points before clustering. The main approach would be to
                    filter the points by checking there pixel value and eliminate any that aren't green enough(most
                    likely noise instead of a plant part). We attempted to implement this preprocessing with trial/error
                    but no success, would require more research.
                    <br><br>
                    Using the true centers found by the clustering, subimages are extracted to feed into the model. A
                    formula was created to find the size of the subimage output based on the input image size and the
                    number of objects detected in the image. A rectangle of the specified size is placed around each
                    center thus creating the boundaries of the subimage.
                    <br><br>
                    Finally, these segmented image for identified plants would be sent to the classification model to
                    predict the type of plant. Each image will then be labelled as one of the possible plants as
                    discussed previously, or unidentifiable herb if the confidence score for the prediction is lower
                    than a pre-determined threshold.
                </p>

                <h3 class="mb-0">Dataset</h3>
                <p class="lead mb-5">For this project, we were unable to find an already existing dataset. Therefore, we
                    will be creating our own dataset for the training & testing of model from Google Images. To create
                    the training & testing set we will pull 500-1000 images from Google Images for each of our four
                    herb types: parsley, thyme, chives, and oregano, and split the set into 90% and 10% to train &
                    test.
                    <br><br>
                    For the final test set we will be pulling 20-30 images of herb gardens. We will manually label these
                    images to test the prediction of our trained model in presence of multiple herbs in a single image.
                    Our main goal is to segment these herb garden images to segment into different herb-leaves region
                    and then predict their type in the image and label them.
                </p>
            </div>
        </section>
        <!-- <hr class="m-0" /> -->
        <!-- Implementation-->
        <!-- <section class="resume-section" id="Implementation">
            <div class="resume-section-content">
                <h2 class="mb-0">Implementation Overview</h2>
                <p class="lead mb-5">
                    We programmed a script that extracted 240 images from google for each of the four classes, parsley,
                    thyme, chives,
                    and oregano. The images were preprocessed to 224 by 224 and split into train/test sets. The train
                    set was then used to train a <i>MobileNet</i> model with a custom output layer. The model has a
                    ~95% accuracy on the test set.
                    <br><br>
                    The caveat with this approach is that we have not manually inspected the dataset of images,
                    therefore our model may have some misrepresented images. For example, when looking at the labeled
                    images for parsley you will find some that are an image of prepackages dried parsley including the
                    text "Parsley". We believe this error in our dataset might be causing our model to overfit,
                    potentially affecting negatively on the ability to correctly identify the plant after segmentation
                    in the future. This is something we will have to revisit once we start feeding the segmented image
                    into the model.
                    <br><br>
                    21 images were then extracted from Instagram to use as the test set for the image segmentation.
                    Instagram was used as it was a reliable method to get test images that best resemble being taken
                    from a natural environment with a normal phone camera. For labeling purposes we choose images that
                    included the gardener labeling the plants, this eliminated the need for the three of us, who are not
                    herb experts from misidentifying the plants. This once again leads to the caveat that we don't want
                    the herb names in our test images as we believe it has potential for our model to incorrectly learn
                    the name of the plants instead of the leafs identifying feature. The major goal of our project is to
                    take an herb garden where you don't know what is what and label it, therefore in the pre-processing
                    of the test images we blurred out any identifying labels of the herbs in the image. The removal of
                    text will be referred as Stage 1.
                    <br><br>
                    In stage 2 the image is then proccessed to create a dataset of object centers. During this stage
                    noise is removed from the image and then the object centers are annotated onto an image. When
                    annotating these centers on the images we find that there tends to be clusters of ”centers” around
                    each plant.
                    <br><br>
                    Stage 3 encompasses the major task of locating the approximate true center of each plant using
                    the points found in stage 2. As this stage, involves a big part of the novilty of our project, there are improvements to be made. Before implementing this 
                </p>
            </div>
        </section> -->
        <hr class="m-0" />
        <!-- Stage 1-->
        <section class="resume-section" id="Stage1">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 1: Blurring Text</h2>
                <p class="lead mb-5">
                    Each image will go through three steps to blurr any text:[4]
                <ol>
                    <li>Identify text in the image and obtain the bounding box coordinates of each text, using
                        Keras-ocr.</li>
                    <li>For each bounding box, apply a mask to tell the algorithm which part of the image we should
                        inpaint.</li>
                    <li>Apply an inpainting algorithm to inpaint the masked areas of the image, resulting in a text-free
                        image, using cv2.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/input.png" alt="Original Image" height="400">
                        <div>Stage 1 Input</div>
                    </div>
                    <div>
                        <img src="./assets/img/noWordsOutput.png" alt="Image with all text Blurried" height="400">
                        <div>Stage 1 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 2-->
        <section class="resume-section" id="Stage2">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 2: Annotate "Centers"</h2>
                <p class="lead mb-5">
                    The image will be processed to create a dataset of object centers to be processed in Stage 3.The is
                    done in four steps:
                <ol>
                    <li>Apply a specified blur filter to the image using skimage.filters. Possible
                        options are Gaussian, median, bilateral, and total variation. We choosing Gaussian based off of
                        trying them and comparing the annotated images.</li>
                    <li>Convert the blurred image to a binary image using skimage.filters.threshold_otsu. Threshold was
                        also based off of trying values and comparing the annotated images.</li>
                    <li>Label connected components in the binary image and compute region properties for each object
                        using measure.label and measure.regionprops respectively. </li>
                    <li>For each plant, plot a red circle on the centroid coordinates using matplotlib.pyplot.plot.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/annotateCenterOutput.png" alt="Image with red dots." height=400>
                        <div>Stage 2 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 3-->
        <section class="resume-section" id="Stage3">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 3: Annotate True Centers</h2>
                <p class="lead mb-5">
                    In this stage, the "centers" found in stage 2 are processed using K-means to find the true center of
                    each plant. Unfortunately, this stage currently requires manual input for the value of K. In the
                    future, we would like to find a clustering method that does not require human input to determine the
                    optimal number of clusters.
                </p>
                <img src="./assets/img/centerOutput.png" alt="Example image of stage three output." height=400>
                <div>Stage 3 Output</div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 4-->
        <section class="resume-section" id="Stage4">
            <div class="resume-section-content">
                <h2 class="mb-0">Stage 4: Extract Subimages Image</h2>
                <p class="lead mb-5">
                    In Stage 4 using the true centers found in stage 3, we extract surrounding pixels to create
                    subimages of each plant.

                    The formula used to calculate the size of the subimages is based off of the original image size and
                    the number of plants in the image.

                </p>
                <p><span class="math display">\[output_{size} = \sqrt{\frac{input\_img_{height} *
                        input\_img_{width}}{num\_of\_plants * 1.5}}\]</span></p>
                <p class="lead mb-5">
                    For each x,y cordinate that indicates a plant center a box boundry is created using the following
                    formula:
                </p>
                <p><span class="math display">\[x_{min} = \max(0,x - \lfloor \frac{output_{size}}{2} \rfloor) \]</span>
                </p>
                <p><span class="math display">\[x_{max} = \min(input\_img_{width},x + \lfloor \frac{output_{size}}{2}
                        \rfloor) \]</span></p>
                <p><span class="math display">\[y_{min} = \max(0,y - \lfloor \frac{output_{size}}{2} \rfloor) \]</span>
                </p>
                <p><span class="math display">\[y_{max} = \min(input\_img_{height},y + \lfloor \frac{output_{size}}{2}
                        \rfloor) \]</span></p>
                <p class="lead mb-5">
                    The box boundries are used to extract the subimages.
                </p>
                <div>Stage 4 Output</div>
                <table>
                    <tr>
                        <td><img src="assets/img/subsetImages/subset_0.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_1.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_2.png" width=200></td>
                    </tr>
                    <tr>
                        <td><img src="assets/img/subsetImages/subset_3.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_4.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_5.png" width=200></td>
                    </tr>
                    <tr>
                        <td><img src="assets/img/subsetImages/subset_6.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_7.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_8.png" width=200></td>
                    </tr>
                    <tr>
                        <td><img src="assets/img/subsetImages/subset_9.png" width=200></td>
                        <td><img src="assets/img/subsetImages/subset_10.png" width=200></td>
                    </tr>
                </table>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Discussion-->
        <section class="resume-section" id="Improvements">
            <div class="resume-section-content">
                <h2 class="mb-0">Improvements</h2>
                <p class="lead mb-5">
                    One of the main limitation of our current set up is that we only train our model on four herbs, when
                    in reality there are thousands of different herbs. At minimum it would be great to expand the model
                    to at least cover all of the commonly used spices and herbs used world wide in cooking. This would
                    make the application in the food industry much more useful.
                    <br></br>
                    In an ideal world, we would also use completely different images in training our model. Ideally we
                    would have enough pre-labeled images of herb gardens that would allow us to segment the image and
                    use the sub-images as the train and test set in our model as the model currently uses high quality
                    images of the herbs usually against a white background, this doesn't match the segmented images we
                    are feeding into the model for labeling.
                    <br></br>
                    Another challenge that we aim to tackle if time permits is to successfully segment the mixed leaf
                    scenario and identify the plants. In reality, the image of a garden could contain the interleaved
                    leaves into each other if plants are nearby. Current approach might find it difficult to segment
                    this scenario into two or multiple separate images. Hence, it would be a challenging task to develop
                    a model for identification of leaves in such a case.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Resources-->
        <section class="resume-section" id="Resources">
            <div class="resume-section-content">
                <h2 class="mb-0">Resources</h2>
                <ul class="lead mb-5">
                    <li>
                        <a href="https://github.com/aklabjan/herbDetectionProject">Github Repository</a>
                    </li>
                    <li>
                        <a
                            href="https://docs.google.com/presentation/d/15zjstpBCiju-LM5Yy18rcPJ7A-Ig731SxNGlmBvt9vg/edit?usp=sharing">Presentation</a>
                    </li>
                </ul>
            </div>
        </section>
        <!-- References-->
        <section class="resume-section" id="References">
            <div class="resume-section-content">
                <h2 class="mb-0">References</h2>
                <ol class="lead mb-5">
                    <li>
                        <div id="ref-8687165" class="csl-entry" role="doc-biblioentry">
                            Liu, Shupeng, Weiyang Chen, and Xiangjun Dong. 2018. <span>“Automatic
                                Classification of Chinese Herbal Based on Deep Learning Method.”</span>
                            In <em>2018 14th International Conference on Natural Computation, Fuzzy
                                Systems and Knowledge Discovery (ICNC-FSKD)</em>, 235–38. <a
                                href="https://doi.org/10.1109/FSKD.2018.8687165">https://doi.org/10.1109/FSKD.2018.8687165</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-8350775" class="csl-entry" role="doc-biblioentry">
                            Mareta, Affix, Indah Soesanti, and Oyas Wahyunggoro. 2018. <span>“Herbal
                                Leaf Classification Using Images in Natural Background.”</span> In
                            <em>2018 International Conference on Information and Communications
                                Technology (ICOIACT)</em>, 612–16. <a
                                href="https://doi.org/10.1109/ICOIACT.2018.8350775">https://doi.org/10.1109/ICOIACT.2018.8350775</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-9239938" class="csl-entry" role="doc-biblioentry">
                            Muneer, Amgad, and Suliman Mohamed Fati. 2020. <span>“Efficient and
                                Automated Herbs Classification Approach Based on Shape and Texture
                                Features Using Deep Learning.”</span> <em>IEEE Access</em> 8: 196747–64.
                            <a
                                href="https://doi.org/10.1109/ACCESS.2020.3034033">https://doi.org/10.1109/ACCESS.2020.3034033</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-removeText" class="csl-entry" role="doc-biblioentry">
                            "Remove Text from Images Using cv2 and Keras-ocr." <em>Towards Data Science</em>. Accessed
                            April 19, 2023. <a
                                href="https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4">https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4</a>.
                        </div>
                    </li>
                </ol>
            </div>
        </section>
        <hr class="m-0" />
    </div>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>