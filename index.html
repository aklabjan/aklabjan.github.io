<!DOCTYPE html>
<html lang="en">
<script type="text/javascript" async=""
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="Herb Garden Classification System" content="CS766 Project" />
    <meta name="Ana Klabjan, Poulami Paul and Shubhankit Rathore" content="CS766 Project" />
    <title>Herb Garden Classification</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
        type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-block d-lg-none">Herb Garden Classification System</span>
            <!-- <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                    src="assets/img/tomatoes.jpeg" alt="..." /></span> -->
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
                class="navbar-toggler-icon"></span></button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Title">Project Details</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#TheProblem">The Problem</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#StateOfTheArt">Current State Of The
                        Art</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Approach/Novelty">Our Approach</a>
                </li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Dataset">Dataset</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage1">Stage 1</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage2">Stage 2</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage3">Stage 3</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage4">Stage 4</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage5">Stage 5</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Stage6">Stage 6</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#results">Results</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Findings">Findings/Attempts</a></li>
                
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Improvements">Future Improvements</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Resources">Resources/Links</a></li>
                <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#References">References</a></li>
            </ul>
        </div>
    </nav>
    <!-- Page Content-->
    <div class="container-fluid p-0">
        <!-- Title and Names-->
        <section class="resume-section" id="Title">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Herb Garden </span>Classification System</h2>
                <p class="lead mb-5">
                    By Ana Klabjan, Poulami Paul and Shubhankit Rathore
                </p>
                <p class="lead mb-5">CS 766-Spring 2023</p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- TheProblem-->
        <section class="resume-section" id="TheProblem">
            <div class="resume-section-content">
                <h2 class="mb-0">
                    <span class="text-primary"> The Problem</span>
                </h2>
                <p class="lead mb-5"> The problem we are trying to solve is to accurately label different herbs in a
                    herb garden when a photo is taken. Specifically, we aim to identify the herbs parsley, thyme,
                    chives, and oregano, while labeling all other herbs as unclassified herb. The goal of this project
                    is to develop a computer vision model that can recognize these specific herbs in an image with high
                    accuracy and efficiency.
                    <br><br>
                    It might be out of curiosity for someone to figure out what kind of herbs they have in their garden
                    or forest. Accurate classification of herb leaves is critical for quality control, authentication,
                    and conservation of medicinal plants. Therefore, the proposed project aims to develop a herb leaf
                    classification system that can identify individual plants within a garden and differentiate between
                    different herb species based on their leaf images.
                </p>
                <h3 class="mb-0">
                    Importance and Motivation
                </h3>
                <p class="lead mb-5">The problem we are trying to solve is important for several reasons. Firstly, it
                    can satisfy the curiosity of people who want to know what kind of herbs they have in their garden or
                    environment. This can be time-consuming and challenging for those without extensive herb knowledge.
                    Secondly, this tool can also be helpful for the food industry, as it can assist with quality control
                    in the production of herb-infused products and reduce the risk of misidentification, which can be
                    dangerous if a misidentified herb is consumed.
                    <br><br>
                    Lastly, accurate identification of herb leaves is crucial for quality control, authentication, and
                    conservation of medicinal plants. The development of a herb leaf classification system that can
                    identify different herb species based on their leaf images can aid in the production of medicines,
                    as well as the identification and preservation of endangered plant species. Additionally, the
                    accurate labeling of herbs in natural environments can provide valuable data for ecological studies,
                    aiding in the conservation and management of natural habitats. Therefore, it is crucial to develop a
                    computer vision model that can accurately and efficiently identify different herb species in images,
                    as it has practical applications for the food industry, herb enthusiasts, medicinal purposes, and
                    environmental research.
                    <br><br>
            </div>
        </section>
        <hr class="m-0" />
        <!-- StateOfTheArt-->
        <section class="resume-section" id="StateOfTheArt">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Current State Of The Art</span></h2>
                <p class="lead mb-5">In recent years, herb classification using leaf images has gained significant
                    attention due to its potential applications in the fields of medicine, agriculture, and food
                    industry. The ability to accurately identify different herb species can help in quality control of
                    herbal products, detection of adulteration, and conservation of rare and endangered plant species.
                    <br><br>
                    Most of the existing studies in this area have focused on the extraction of features from images
                    that represent a singular leaf from an herb and their classification using various machine learning
                    algorithms. Texture features, color histograms, and shape descriptors are some of the commonly used
                    features for herb classification[3]. However, with the recent advancements in deep
                    learning, several studies have started exploring the use of convolution neural networks (CNNs) for
                    herb recognition based on leaf images[1].
                    Leaf identification normally use images that taken in the laboratory with sophisticated equipment
                    and a white background. The accuracy of these models have been high.
                    <br><br>
                    Current research has shifted the focus from high-quality plant images to identifying leaves in
                    natural environments. Leaf segmentation in natural environments has different approaches and
                    accuracy levels. There have been many studies that aim to identify leaves based on images taken in
                    their natural environment[2]. These studies all focused on the identification of a
                    singular leaf taken in nature. The accuracy of the models was lower than the models discussed in the
                    prior paragraph that used images taken with sophisticated equipment and a white background, but the
                    models were still considered successful, with high accuracy.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Method-->
        <section class="resume-section" id="Approach">
            <div class="resume-section-content">
                <h2 class="mb-0"> <span class="text-primary">Our Approach/Novelty</span></h2>
                <p class="lead mb-5">Our project intends to expand on the current state-of-the-art. The research for
                    identifying an individual leaf from nature has already been conducted, and our project expands on
                    it. However, we have not found any studies that take an image of numerous plants and segment the
                    image to locate each individual plant. Each segmented plant would then be fed into an identification
                    model, similar to the pre-existing ones. The original image would then be annotated using the model
                    output.
                    <br><br>
                    For plant segmentation, we have developed our own approach. Firstly, we will identify the center of
                    each individual plant and annotate it on the original image. To do this, we will convert the herb
                    garden image to grayscale, perform some noise removal, compute the edges/boundaries, and then
                    compute the "centers" of each object detected by the edges. Through implementation we found this
                    detected individual leaves more than plants as a whole(reference stage 2). Thus leading us to the
                    conclusion we will
                    need to perform some type of clustering.
                    <br><br>
                    For the clusters, we choose to use K-Means to identify the true plant centers from the noisy output
                    provided by Stage 2. In an ideal world we would like to avoid the need for human input of the value
                    “K” but for the time permitted using k-means provided the best possible results. We considered
                    several clustering algorithms including hierarchical clustering. The caveat was even though there is
                    no original specified number of clusters, you would have to identify a criteria on how to decide
                    where to split the clusters in the hierarchy. For our algorithm we predefined K based on manual
                    inspection of each test image declaring K to be the number of distinct plants visible. Using the
                    output from k-means we have the coordinates of what our algorithm believes the true plant centers
                    are. When plotted on the original image we find that the true plants are mostly identified although
                    there are some plant pots and container edges detected that create noise, a falsely identified
                    plant.
                    <br><br>
                    Using the true centers found by the clustering, subimages are extracted to feed into the model. A
                    formula was created to find the size of the subimage output based on the input image size and the
                    number of objects detected in the image. A rectangle of the specified size is placed around each
                    center thus creating the boundaries of the subimage.
                    <br><br>
                    Finally, these segmented image for identified plants are sent to the classification model to predict
                    the type of plant. Each image is then labelled as one of the possible plants as discussed
                    previously, or unidentifiable herb if the confidence score for the prediction is lower than a
                    pre-determined threshold.
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Dataset-->
        <section class="resume-section" id="Dataset">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Dataset</span></h2>
                <p class="lead mb-5">
                    <br>Creating a dataset for common herbs is a challenging task, primarily because of the difficulty in finding suitable images. While scraping images from the internet may seem like an easy solution, it poses several issues that can affect the quality and accuracy of the dataset. Some of the most common issues include:
                    <ul>
                    <li>Low-quality images: Images of low resolution or poor quality can make it challenging to discern the details of the herb or plant. This can result in inaccurate classification or recognition of the herb by machine learning models.</li>
                    <li>Misleading images: Images of processed herbs or herb products such as dried leaves or herbs in jars may not provide the full context of the herb as a plant, making it difficult to train a model to recognize the plant as a whole.</li>
                    <li>Repetitive and incorrect search results: Scraping images off the internet can result in repetitive and incorrect search results that do not provide the required diversity and quality of images.</li>
                    </ul>
                </br>
                <br>To overcome these challenges, one approach to creating a reliable dataset for common herbs is capturing real bush images. However, this may not always be practical due to limited access to farms or fields. An alternative approach is to obtain stock images via scraping Google images. However, as we discussed, this process requires careful curation and filtering to ensure that the images are of high quality and suitable for the dataset. 
                </br>
                <br>
                </br>
                <br>Gathering and refining data manually for each herb is a time-consuming process that requires significant effort. Additionally, pre-processing of the images is required to ensure uniformity and consistency of the dataset. This is particularly important when dealing with bad quality images that may include low-resolution images or misleading herb images.
                </br>
                <br>We therefore scraped images off google images and stock images from different websites. As mentioned above, it required manual efforts to create a usable dataset out of these imgaes. However, due to these constraints, the size of dataset couldn't be made big, hence limiting the scope of training more accurate models. To illustrate the challenges involved in creating a dataset for common herbs, we have provided two sample images below. The first image shows a low-resolution Oregano bush, while the second image shows a jar of Oregano. These images demonstrate the kind of unusable images that can result from scraping images off the internet. 
                 </br>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/lowres-oregano.jpeg" alt="Low-Resolution Oregano Bush" height="150" width="250">
                        <div>Low-Resolution Oregano Bush</div>
                    </div>
                    <div>
                        <img src="./assets/img/jar-oregano.png" alt="Incorrect image returned as Oregano Jar instead of Oregano Bush" height="150">
                        <div>Oregano Jar</div>
                    </div>
                </div>
                <br>
                Our final dataset contains about 1000 plant bush images for each of the four chosen herbs (Oregano, Parsley, Thyme, and Chives). These images are further divided into Train and test datasets for training our classifier model to identify the type of bush passed to it. Another set of images that we have kept separately, sourced from sources like instagram, which contain different plant herb bushes in a single image and are also labelled. These images would help us test the final result of our tool end to end. However, such images are difficult to locate, and hence are very few just for testing purpose. It has to be noted that, these images are not part of train/validation dataset, and are totally new data fed into our tool for obtaining results.
                </br>
<div class="image-container">
                    <div>
                        <img src="./assets/img/directory.png" alt="Dataset organization" height="150">
                        <div>Dataset organization</div>
                    </div>
                </div>
    <!-- For this project, we were unable to find an already existing dataset. Therefore, we
                    will be creating our own dataset for the training & testing of model from Google Images. To create
                    the training & testing set we will pull 500-1000 images from Google Images for each of our four
                    herb types: parsley, thyme, chives, and oregano, and split the set into 90% and 10% to train &
                    test.
                    <br><br>
                    For the final test set we extracted 20 images of herb gardens. These images were pulled from
                    Instagram as it was a good source of images that resembled the real world input our project is
                    intended for. Creating our own dataset was a big hurdle. In order to eliminate the possibility of
                    incorrectly labeling a plant in the test set when compared to the output from our program we
                    selected images that included labels by the garden. Although this is flawed and has the potential to
                    be incorrectly labeled, we made the assumption the gardener labeled it based on the label of the
                    seed packaging or store label. -->
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 1-->
        <section class="resume-section" id="Stage1">
            <div class="resume-section-content">
                <h2 class="mb-0"> <span class="text-primary">Stage 1 </span>Blurring Text</h2>
                <p class="lead mb-5">
                    Each image will go through three steps to blurr any text:[4]
                <ol>
                    <li>Identify text in the image and obtain the bounding box coordinates of each text, using
                        Keras-ocr.</li>
                    <li>For each bounding box, apply a mask to tell the algorithm which part of the image we should
                        inpaint.</li>
                    <li>Apply an inpainting algorithm to inpaint the masked areas of the image, resulting in a text-free
                        image, using cv2.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/input.png" alt="Original Image" height="400">
                        <div>Stage 1 Input</div>
                    </div>
                    <div>
                        <img src="./assets/img/noWordsOutput.png" alt="Image with all text Blurried" height="400">
                        <div>Stage 1 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 2-->
        <section class="resume-section" id="Stage2">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Stage 2 </span>Annotate "Centers"</h2>
                <p class="lead mb-5">
                    The image will be processed to create a dataset of object centers to be processed in Stage 3.The is
                    done in four steps:
                <ol>
                    <li>Apply a specified blur filter to the image using skimage.filters. Possible
                        options are Gaussian, median, bilateral, and total variation. We choosing Gaussian based off of
                        trying them and comparing the annotated images.</li>
                    <li>Convert the blurred image to a binary image using skimage.filters.threshold_otsu. Threshold was
                        also based off of trying values and comparing the annotated images.</li>
                    <li>Label connected components in the binary image and compute region properties for each object
                        using measure.label and measure.regionprops respectively. </li>
                    <li>For each plant, plot a red circle on the centroid coordinates using matplotlib.pyplot.plot.</li>
                </ol>
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/annotateCenterOutput.png" alt="Image with red dots." height=400>
                        <div>Stage 2 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 3-->
        <section class="resume-section" id="Stage3">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Stage 3 </span> Annotate True Centers</h2>
                <p class="lead mb-5">
                    In this stage, the "centers" found in stage 2 are processed using K-means to find the true center of
                    each plant. Unfortunately, this stage currently requires manual input for the value of K. In the
                    future, we would like to use a clustering method that does not require human input to determine the
                    optimal number of clusters. X-Means is one such algorithm[6]. Due to the current noise after stage 2
                    X-Means does not perform well so we are using K-Means for better results from the segmentation. If
                    we can filter out the noise I believe replacing K-means with X-means would lead to good results and
                    no human input.
                </p>
                <div class="image-container">
                    <div>
                        <img src="./assets/img/centerOutput.png" alt="Example image of stage three output." height=400>
                        <div>Stage 3 Output</div>
                    </div>
                </div>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Stage 4-->
        <section class="resume-section" id="Stage4">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Stage 4 </span> Extract Subimages Image</h2>
                <p class="lead mb-5">
                    In Stage 4 using the true centers found in stage 3, we extract surrounding pixels to create
                    subimages of each plant.

                    The formula used to calculate the size of the subimages is based off of the original image size and
                    the number of plants in the image.

                </p>
                <p><span class="math display">\[subimage_{size} = \sqrt{\frac{image_{height} *
                        image_{width}}{number\_of\_plants * 1.5}}\]</span></p>
                <p class="lead mb-5">
                    For each x,y cordinate that indicates a plant center a box boundry is created using the following
                    formula:
                </p>
                <p><span class="math display">\[x_{min} = \max(0,x - \lfloor \frac{subimage_{size}}{2} \rfloor)
                        \]</span>
                </p>
                <p><span class="math display">\[x_{max} = \min(image_{width},x + \lfloor \frac{subimage_{size}}{2}
                        \rfloor) \]</span></p>
                <p><span class="math display">\[y_{min} = \max(0,y - \lfloor \frac{subimagegit _{size}}{2} \rfloor)
                        \]</span>
                </p>
                <p><span class="math display">\[y_{max} = \min(image_{height},y + \lfloor \frac{subimage_{size}}{2}
                        \rfloor) \]</span></p>
                <p class="lead mb-5">
                    The box boundries are used to extract the subimages.
                </p>
                <div class="image-container">
                    <div>
                        <div>Stage 4 Output</div>
                        <table>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_0.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_1.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_2.png" width=200 height=200
                                        object-fit=contain></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_3.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_4.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_5.png" width=200 height=200></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_6.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_7.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_8.png" width=200 height=200></td>
                            </tr>
                            <tr>
                                <td><img src="assets/img/subsetImages/subset_9.png" width=200 height=200></td>
                                <td><img src="assets/img/subsetImages/subset_10.png" width=200 height=200></td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </section>
<hr class="m-0" />
<!-- Stage 5-->
        <section class="resume-section" id="Stage5">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Stage 5 </span> Predict type of herb using a CNN model</h2>
                <p class="lead mb-5">
                   <br>
                    In this stage, we feed the segmented images from previous stage as an input to our trained custom CNN model. We trained our machine learning model on each plant's image in our dataset separately, rather than combined images. We also tried to use transfer learning on a pre-trained model by fine-tuning it over our new dataset. Specifically, we attempted to use the MobileNet architecture, which has showed promise in previous classification studies. However, we encountered issues with this approach as the MobileNet model resulted in overfitting on our dataset. This means that while the model achieved high accuracy during training (up to 99.8%), it failed to generalize to new images. This is a common problem when using complex models on small datasets.
                    </br>
<br>
To address this issue, we decided to redeclare a new CNN (Convolutional Neural Network) model with fewer layers (approximately 9 layers). This helped to solve the overfitting problem to some extent and improved the model's ability to generalize to new herb bush images.
</br>
<div class="image-container">
                    <div>
                        <div>Custom CNN Model layers</div>
                            <tr>
                                <td><img src="assets/img/cnn.png" height=200></td>
                                
                            </tr>

                    </div>
                </div>
<br>
Our model provides softmax probabilities for each class, which allows us to label an image as one of the known herb plants or label it as unidentified if the probability is below a set threshold. This gives us greater flexibility in how we use the model and allows us to detect previously unseen herb plants. The classes for model can easily be expanded and the model can also be re-trained over new herb plants based on available dataset.
</br>
<br>
Overall, our new CNN model provides better prediction for new herb bush images and shows promise in herb recognition. By leveraging our unique dataset of common herb plants, we are able to train a model that is tailored to our specific needs and can provide better classification of herbs in real-world scenarios.
</br>

<div class="image-container">
                    <div>
                        
                            <tr>
                                <td><img src="assets/img/softmax.png" width=400></td>
                                <div>Output Probabilities of model</div>
                            </tr>

                    </div>
                </div>
                    
                </p>
    
            </div>
        </section>

<!-- Stage 6-->
        <section class="resume-section" id="Stage6">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Stage 6</span> Labelling Predicted herbs in Cluster</h2>
                <p class="lead mb-5">
                   <br>
                    In the concluding phase, we utilize the softmax probabilities generated by the CNN model for each sub-image in the previous stage as the input. The class with the highest softmax probability determines the final label for each sub-image. If the maximum probability value falls below a certain threshold, the label is designated as "Undefined". After extracting these labels, we draw a bounding box around each herb cluster in the original image and annotate them with the generated labels. The bounding box information is directly passed on to this stage from Stage-4, and Probabilities from Stage-5. The final output for this stage is shown in the next section (Results).
                    </br>
                    <div class="image-container">
                    <div>
                        
                            <tr>
                                <td><img src="assets/img/boxlabel.png" width=150></td>
                                <div>Labelled box drawn around plant</div>
                            </tr>

                    </div>
                </div>
                </p>
    
            </div>
        </section>
<hr class="m-0" />
        <!-- Results-->
        <section class="resume-section" id="results">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Results</span></h2>
                <p class="lead mb-5">
                    <table>
                            <tr>
                                <td>
                        <img src="./assets/img/res/input.png" alt="Labeled input test image for mixed herbs" width="250">
                        <div>Pre-labeled input test image for mixed herbs</td>
                    
                        <td><img src="./assets/img/res/ocr.png" alt="Labeled removed input test image" width="250">
                        <div>Labels removed using ocr techniques</div>
                            </td>
                   <tr><td>
                        <img src="./assets/img/res/center.png" alt="Centers annotated on test image" width="250">
                        <div>Centers annotated for each detected plant</div>
                    </td>
                    <td>
                       <img src="./assets/img/res/final.png" alt="Final Result" width="250">
                        <div>Final result labelling each detected herb</div>
                       </td>        
                        
                </tr>
                </table>
                </p>
            </div>
        </section>


        <hr class="m-0" />
        <!-- Findings-->
        <section class="resume-section" id="Findings">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Findings/Attempts</span></h2>
                <p class="lead mb-5">
                    Creating a dataset for common herbs is a challenging task, primarily because of the difficulty in finding suitable images. We encountered low-quality images, misleading images, as well as incorrect and repetitve search results. Hence, a manual inspection and curation of the dataset was essential. This was a very time consuming task which took a major chunk of timeline.
                     <br></br>
                     
                   Once the dataset creation was completed, we proceeded with the image processing segment of  our project. A need to improve noise removal in the process of extracting plant
                    sub-images was identified. In stage 2, basic noise removal was performed using Gaussian blur and
                    thresholding to convert the image to a binary format. However, this approach produced a significant
                    amount of noise in the output, including dots on non-leaf areas.
                    <br></br>
                    Two attempts were made to address this issue, but both failed and required further investigation. The
                    first approach involved filtering the output produced by Stage 2 before using it as input for
                    K-Means in Stage 3. The idea behind this approach was to identify dots associated with actual plants
                    by looking for some variety of green color, while ignoring dots that were unlikely to be part of a
                    plant (such as a pot or a pole).
                    <br></br>
                    The filtering process involved looking up the pixel value of a (X, Y) coordinate in the image,
                    computing the distance to a predetermined value (green_color), and checking if the distance was
                    within a threshold. If the pixel value was close enough to the color of a leaf, it was included in
                    the Stage 3 clustering computation. Otherwise, it was filtered out. An outline can be found below:
                </p>
                <img src="./assets/img/filter.png" width=700>
                <p class="lead mb-5">
                    However, this approach had a major limitation in that it required a better way to determine the
                    value of green_color. Ideally, green_color would be the average value of all pixels in the image
                    that correspond to being part of a plant. However, this value may not be uniform across multiple
                    images due to various factors such as lighting. Further research is needed to determine the correct
                    value for green_color in order to make this filter effective.
                    <br></br>
                    Another attempt to address the issue of noise was made by exploring the possibility of filtering the
                    sub-images produced by Stage 4. The idea behind this attempt was similar to the previous approach,
                    based on the hypothesis that sub-images of plants and sub-images of noise would contain
                    distinguishable pixel values. A sub-image of a plant would typically contain a large number of green
                    pixels, while a sub-image of noise would have few or no green pixels.
                    <br></br>
                    This approach was still in the exploration phase and had not been implemented. The initial
                    exploration involved analyzing the green channel of each sub-image. For each image, the average
                    value of the green channel was calculated for each sub-image. It was found that sub-images
                    containing plants tended to have a similar average value within a range, while the average value of
                    sub-images containing noise fell outside that range. For example, when the average pixel value of
                    the green channel was calculated for each sub-image under Stage 4, the output was as follows:
                    <br></br>
                    Plant Sub-Images = [158.21, 162.95, 150.95, 155.71, 156.73, 163.50, 150.59, 163.27, 148.78]
                    <br></br>
                    Noisy Sub-Images = [184.56, 142.91]
                    <br></br>
                    As shown in this output, sub-images with an average green channel pixel value between 148-163 were
                    identified as containing plants, while sub-images with an average green channel pixel value outside
                    this range matched up with the two images of the white posts.
                    <br></br>
                    However, the average value range of the green channel varied from image to image due to factors such
                    as lighting, which meant that each image would need to have its own range. This led to further
                    exploration, including plotting histograms of pixel values for each color channel on every
                    sub-image. Although a lot of information was present in these histograms, there were no distinct
                    features that stood out.
                </p>
<p class="lead mb-5">
The classification is performed by a trained CNN model over our generated dataset. We  tried to use transfer learning on a pre-trained model (Mobilenet) by fine-tuning it over our new dataset. However, we encountered issues with this approach as the MobileNet model resulted in overfitting on our dataset. It achieved high training accuracy but it failed to generalize to new images. This is a common problem when using complex models on small datasets. Hence, we created a custom CNN model based on the existing classification architectures which was better suited for our small dataset. This helped us solve the overfitting problem to some extent and improved the model's ability to generalize to new herb bush images providing better classification accuracy. 
</p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Discussion-->
        <section class="resume-section" id="Improvements">
            <div class="resume-section-content">
                <h2 class="mb-0"><span class="text-primary">Future Improvements</span></h2>
                <p class="lead mb-5">
                    <ul>
                    <li>One of the main limitation of our current set up is that we only train our model on four herbs, when
                    in reality there are thousands of different herbs. At minimum it would be great to expand the model
                    to at least cover all of the commonly used spices and herbs used world wide in cooking. This would
                    make the application in the food industry much more useful.
                    </li>
                    <br></br>
                        <li>To improve the robustness of our CNN model for herb classification, it would be ideal to have a more diverse dataset with completely different images. An annotated dataset with pre-labeled images of herb gardens would allow us to segment the images and use sub-images as the train and test set in our model, enabling us to train more robust CNN models or even utilize R-CNNs for classification. However, creating such a dataset can be challenging and time-consuming. Despite these challenges, having a more diverse and annotated dataset would greatly enhance the accuracy and reliability of our herb classification models.</li>
                    
                    <br></br>
            <li>Exploring better ways to determine the true centers of plants in order to avoid noisy segmentations. This will help ensure that our models are accurately recognizing and classifying the plants in the images. We also plan to explore methods for determining the actual number of plants in the image and segmenting them, which could be achieved using R-CNNs instead of traditional computer vision algorithms. However, this will require a curated annotated dataset, which will need to be created.</li>
            <br></br>
            <li>
                    In the future, we could explore the challenge of segmenting mixed herb plant images and accurately identifying each plant. This challenge is significant as mixed herb plant images are common in real-world scenarios. Successfully segmenting mixed herb plant images and identifying the different plants in such scenarios would require exploring various techniques for image segmentation and developing models that can recognize the unique features of each plant. By developing a model that can accurately identify the different plants in mixed images, we can improve the robustness of our herb classification models and enable them to recognize and classify herbs accurately even in complex real-world scenarios.
                </li>
            </ul>
                </p>
            </div>
        </section>
        <hr class="m-0" />
        <!-- Resources-->
        <section class="resume-section" id="Resources">
            <div class="resume-section-content">
                <h2 class="mb-0">Resources</h2>
                <ul class="lead mb-5">
                    <li>
                        <a href="https://github.com/aklabjan/herbDetectionProject">Github Repository</a>
                    </li>
                    <li>
                        <a
                            href="https://docs.google.com/presentation/d/1HoOTOZAkJhqP111lgKF9xl0XaGWU28aQP1xfZ8hRzCA/edit?usp=sharing">Presentation</a>
                    </li>
                    <li>
                        <a href="/assets/Midterm_Report.pdf">Midterm Report</a>
                    </li>
                </ul>
            </div>
        </section>
        <!-- References-->
        <section class="resume-section" id="References">
            <div class="resume-section-content">
                <h2 class="mb-0">References</h2>
                <ol class="lead mb-5">
                    <li>
                        <div id="ref-8687165" class="csl-entry" role="doc-biblioentry">
                            Liu, Shupeng, Weiyang Chen, and Xiangjun Dong. 2018. <span>“Automatic
                                Classification of Chinese Herbal Based on Deep Learning Method.”</span>
                            In <em>2018 14th International Conference on Natural Computation, Fuzzy
                                Systems and Knowledge Discovery (ICNC-FSKD)</em>, 235–38. <a
                                href="https://doi.org/10.1109/FSKD.2018.8687165">https://doi.org/10.1109/FSKD.2018.8687165</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-8350775" class="csl-entry" role="doc-biblioentry">
                            Mareta, Affix, Indah Soesanti, and Oyas Wahyunggoro. 2018. <span>“Herbal
                                Leaf Classification Using Images in Natural Background.”</span> In
                            <em>2018 International Conference on Information and Communications
                                Technology (ICOIACT)</em>, 612–16. <a
                                href="https://doi.org/10.1109/ICOIACT.2018.8350775">https://doi.org/10.1109/ICOIACT.2018.8350775</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-9239938" class="csl-entry" role="doc-biblioentry">
                            Muneer, Amgad, and Suliman Mohamed Fati. 2020. <span>“Efficient and
                                Automated Herbs Classification Approach Based on Shape and Texture
                                Features Using Deep Learning.”</span> <em>IEEE Access</em> 8: 196747–64.
                            <a
                                href="https://doi.org/10.1109/ACCESS.2020.3034033">https://doi.org/10.1109/ACCESS.2020.3034033</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-removeText" class="csl-entry" role="doc-biblioentry">
                            "Remove Text from Images Using cv2 and Keras-ocr." <em>Towards Data Science</em>. Accessed
                            April 19, 2023. <a
                                href="https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4">https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-1" class="csl-entry" role="doc-biblioentry">
                            "Deep leaf: Mask R-CNN based leaf detection and segmentation from digitized herbarium
                            specimen images." <em>Pattern Recognition Letters</em>, vol. 150, no. 1, 2021, pp. 88-95. <a
                                href="https://doi.org/10.1016/j.patrec.2021.07.003">https://doi.org/10.1016/j.patrec.2021.07.003</a>.
                        </div>
                    </li>
                    <li>
                        <div id="ref-2" class="csl-entry" role="doc-biblioentry">
                            Novikov, A., 2019. PyClustering: Data Mining Library. <em>Journal of Open Source
                                Software</em>, 4(36), p.1230. Available at: <a
                                href="http://dx.doi.org/10.21105/joss.01230">http://dx.doi.org/10.21105/joss.01230</a>.
                        </div>
                    </li>
                </ol>
            </div>
        </section>
        <hr class="m-0" />
    </div>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>